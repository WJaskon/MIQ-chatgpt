TITLE
A Preliminary Study for Detecting Visual Search Behaviors During Street Walking Using Earable Device

ABSTRACT
Map applications on smartphones are powerful navigation tools for walking among places to visit for the first time and are widely used. On the other hand, checking the map applications tend to cause accidents on the road such as collisions with people, cars, and objects. To prevent this, we need to detect a walker’s context regarding visual search behaviors and provide appropriate navigational information to the walker. In this paper, we propose a method to detect a walker’s context regarding visual search behaviors by using motion sensors on an earable device. We collected and investigated motion and gaze data from an earable device and a gaze tracker respectively during street walking from five participants. Based on the investigation, we created a machine learning model for detecting looking around, smartphone, or normal during walking and stopping conditions. Our evaluations showed that our models can detect more than 95% walking and stopping conditions, and 71% of three detail conditions during walking, respectively.

CCS CONCEPTS
• Human-centered computing → Ubiquitous and mobile devices.

KEYWORDS
Earable device, mobile sensor, behavior recognition, navigation, machine-learning

1 INTRODUCTION 
In recent years, wearable devices are being widely used on the arm, head, or abdomen [4, 9, 11]. Using the different sensors mounted on such devices, it has become possible to easily, and in detail, detect and collect information from daily activities, such as heart rate, number of steps, and calorie expenditure. In addition, wearable devices have become smaller, lighter, and less expensive. It is anticipated that we will use wearable devices in daily life more than ever before. Among wearable devices, those that are worn on the ear are called "Earable Devices" [4]. When earable devices are equipped with a motion sensor and a microphone and when these capabilities are combined with voice recognition functions on a smartphone, a new hands-free user assistance service can be realized [8]. For example, if earable devices can detect a walker lost in a city, it can be possible to realize efficient and safe route guidance by combining it with voice guidance. On the other hand, to realize this service, a high degree of accuracy in behavior recognition is important. The purpose of this study is to verify whether data collected from motion sensors mounted on earable devices can be used to correctly detect a walker’s state while street walking. If the walker’s walking condition can be correctly detected using earable devices, it will lead to the realization of new services such as voice navigation at the appropriate time. The contributions of this paper are as follows:
 • The feasibility of using earable devices in detecting a walkers’ visual search behavior during street walking is explored. 
• Motion-sensor data are labeled using Tobii’s eye gaze recordings. 
• Machine learning models are used to classify whether a walker is walking or stopping from motion-sensor data collected by earable devices.

2 RELATED WORKS 
Research has been conducted to estimate user behavior by using sensors in wearable devices, in addition to smartphones. Earable devices, which are devices worn on ears like AirPods, are expected to become important in the development of wearable devices in the future. Earable devices will provide various services such as continuous sensing of human behavior, the realization of AI (augmented reality) through sound, information transmission by AI voice assistants such as Alexa and Siri, and tracking of health status. To recognize behavioral and physical activities through earable devices,he aggregation of collected open data is essential. In 2018, Nokia Bell Labs at the University of Cambridge developed eSense [4], a research and development platform, to facilitate the activation of research on earable devices. Several studies have focused on behavior recognition through earable devices [1–3, 5]. Compared with a smartphone, which is shaken in the pocket, and a smartwatch, which is affected by arm swing, an earable device is more stable due to being on the ear. The accuracy of detecting head movement with earable devices using just an accelerometer has shown to be higher than using both an accelerometer and a gyroscope together [6]. These results indicate the usefulness of sensing design in earable devices. In addition, by leveraging the superiority of earable devices in accurately tracking the state of a head, some studies have shown that an inertial navigation system using an inertial measurement unit in an earable device could enable voice navigation on behalf of a visually impaired person [1].

3 VISUAL SEARCH DETECTION 
In this section, we present a detailed framework and classification method. 

3.1 Overview 
Figure 1 illustrates the overview of our approach for detecting visual search behaviors while walking in a city. As illustrated, we designed a two-phase approach for detecting visual search behaviors. First, our method detects whether a walker is walking or has stopped. After that, our method classifies three types of visual search behaviors: look around, look hands (i.e., smartphone), and normal. We categorized the walker’s visual search behaviors as follows: First, we classified whether participants were walking or stopping, and each of the two states were further subdivided into three states. As a result, six types of states were defined.
• walking (normal walking.) 
• look_sp_while_walking (participant walking while using their smartphone.) 
• look_around_while_walking (participant looking around while walking.) 
• stop (participant standing, such as waiting for a traffic light.)
•look_sp (participant standing still while using their smartphone.) 
• look_around (participant standing still while looking around.)
 
3.2 Devices 
Participants wore AirPods Pro1, as it is an earable device with a motion sensor to detect a walker’s state. AirPods Pro is equipped with an accelerometer and a gyro sensor that can collect motion data in real-time via the iOS CoreMotion API. In this experiment, motion data from AirPods Pro was continuously collected and stored by using the AWARE Framework for iOS 2 [7]. We collected motion sensor data at 30 Hz and saved it in CSV format. In addition to AirPods Pro, subjects wore Tobii Pro Glasses 3 [10] to track the walker’s gaze in real time because this experiment required classification and labeling of the walker’s gaze during street walking. Tobii Pro Glasses 3 is an eye tracking device that is widely used in visual-related scientific research. Figure 2 shows an example of a participant wearing AirPods Pro and Tobii Pro Glasses 3.

3.3 Classification Method 
For the classification of two states, we created multiple machine learning methods. Specifically, we usd five types of machine learning models: K-nearest neighbor method, logistic regression, gradient boosting, random forest, and multi-class support vector machine (SVM). These machine learning models were implemented using Scikit-learn3. In addition, we performed cross-validation on all the machine learning models, thereby creating five patterns with one of the five subjects as the test data and the other four as the training data. We then evaluated each evaluation index using the average value.

4 EVALUATION
In this section, we describe the data collection process, and then we evaluate the feasibility of detecting the user’s state using motionsensor data obtained from an earable device. 

4.1 Data Collection
Five male college students participated in the data collection process (IRB-approved). Each participant wore the devices, as shown in Figure 2, and walked along the designated route. Figure 3 shows the route of the data collection, which is located in an intricately laid out residential neighborhood with streets that are hard to learn to navigate correctly. Six destinations were set between the start and finish points and the walking time was designed to take approximately 20 minutes. Participants were given a map of the route through six destinations and they were asked to walk the route accordingly. Over 20 minutes of walking, we collected the motion data and eye gaze recordings of participants. By having the participants walk along the route, we aimed to collect motion data as naturally as possible. Figure 4 shows an example image of video and gaze recordings using Tobii Pro Glasses 3. 

4.2 Evaluation 1: Detecting Walking and Stopping Conditions
First, we counted the number of each of the two states per subject. The highest number was approximately 26,000 samples of motionsensor data and the lowest was approximately 1,300. To align with the lowest number, 1,280 samples were randomly selected for each subject state. We divided the data into 64 samples and calculated the mean value, variance, maximum value, minimum value, and standard deviation of each sample. Walking and stopping could be precisely classified by of five types of machine learning models. The f1-scores are shown in Table 1. It is apparent taht the random forest model is most suitable for use. The values of the random forest model are 0.95, 0.97, 0.95, and 0.95 in the order of accuracy, precision, recall, and f1-score.

4.3 Evaluation 2: Detecting Detailed Conditions During Walking and Stopping
After classifying the data as walking or stopping, the next step was to classify the motion-sensor data in detail. There are three detailed states to walking, as defined in the previous section: walking, look_sp_while_walking, and look_around_while_walking. We estimated the number of each of the three states per subject. The data of one of the subjects were much less than others; therefore, we used only four subjects’ data. We used the five types of machine learning models. 512 samples were randomly selected, and we divided the data into 64 samples. Regarding stopping, classifying into these three states: stop, look_sp, look_around. We evaluated the data in the same way. 128 samples were randomly selected and divided into 16 samples. The results of classifying each walking and stopping into three states were not sufficient for the effectiveness of the machine learning model. About the result of classifying walking into three detailed states, the random forest model exhibited the best performance: the f1-score was 0.66, and the recall was 0.71. The recall values are shown in Figure 6. All of the machine learning models performed poorly while classifying the stopping condition. The results of the random forest model, the best performer, are shown in Figure 7.

5 DISCUSSION 
There are two reasons why all machine learning models performed poorly in classifying each walking and stopping into three detailed states. First, the amount of data is not adequate for the experiment. The number of subjects need to be increased to gather more motion data. In particular, the stopping time was much shorter than walking time. Therefore, it may be necessary to collect data not only in a natural state but also in a controlled state. Second, we should reconsider the execution of the machine learning models. We used data picking randomly. However, it is very important to deal with the waveform as a characteristic. For example, when subjects turn around, a unique waveform of the rotation sensor recordings appears. Therefore, the method of using a sliding window or bump detection may be effective for keeping the characteristics of the motion data.

6 CONCLUSION 
Focusing on the realization of navigation services for street walking with earable devices, we investigated the possibility of classifying user visual search behavior. We evaluated the feasibility of classifying a walker’s visual search behavior by conducting a preliminary study. The results showed that whether a subject is walking or stopping can be detected by machine learning models and that the random forest model especially had a high performance of 0.95 and approximately 1.0 in terms of f1-score and recall, respectively. However, the classification of walking and stopping remains a problem, and it is necessary to realize more detailed classification. We will collect a larger amount of data and find a more appropriate method for detection in the future. Also, we will provide appropriate navigation information based on a walker’s context detection.

ACKNOWLEDGMENTS
This work was partly supported by NICT, Japan (01101). 

REFERENCES
[1] Ashwin Ahuja, Andrea Ferlini, and Cecilia Mascolo. 2021. PilotEar: Enabling InEar Inertial Navigation. In Adjunct Proceedings of the 2021 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2021 ACM International Symposium on Wearable Computers (Virtual, USA) (UbiComp ’21). Association for Computing Machinery, New York, NY, USA, 139–145. https: //doi.org/10.1145/3460418.3479326 
[2] Md Islam, Tahera Hossain, Md. Atiqur Rahman Ahad, and Sozo Inoue. 2021. Exploring Human Activities Using eSense Earable Device. 169–185. https://doi. org/10.1007/978- 981- 15- 8944- 7_11 
[3] F. Kawsar, C. Min, A. Mathur, and A. Montanari. 2018. Earables for PersonalScale Behavior Analytics. IEEE Pervasive Computing 17, 03 (jul 2018), 83–89. https://doi.org/10.1109/MPRV.2018.03367740 
[4] Fahim Kawsar, Chulhong Min, Akhil Mathur, Alessandro Montanari, Utku Günay Acer, and Marc Van den Broeck. 2018. ESense: Open Earable Platform for Human Sensing. In Proceedings of the 2018 ACM International Joint Conference and 2018 International Symposium on Pervasive and Ubiquitous Computing and Wearable Computers (Singapore, Singapore) (UbiComp ’18). Association for Computing Machinery, New York, NY, USA, 381–383. https://doi.org/10.1145/3267305.3267640 
[5] Seungchul Lee, Chulhong Min, Alessandro Montanari, Akhil Mathur, Youngjae Chang, Junehwa Song, and Fahim Kawsar. 2019. Automatic Smile and Frown Recognition with Kinetic Earables. In Proceedings of the 10th Augmented Human International Conference 2019 (Reims, France) (AH2019). Association for Computing Machinery, New York, NY, USA, Article 25, 4 pages. https: //doi.org/10.1145/3311823.3311869 [6] Chulhong Min, Akhil Mathur, and Fahim Kawsar. 2018. Exploring Audio and Kinetic Sensing on Earable Devices. In Proceedings of the 4th ACM Workshop on Wearable Systems and Applications (Munich, Germany) (WearSys ’18). Association for Computing Machinery, New York, NY, USA, 5–10. https://doi.org/10.1145/ 3211960.3211970 
[7] Yuuki Nishiyama, Denzil Ferreira, Yusaku Eigen, Wataru Sasaki, Tadashi Okoshi, Jin Nakazawa, Anind K Dey, and Kaoru Sezaki. 2020. iOS Crowd-Sensing Won’t Hurt a Bit!: AWARE Framework and Sustainable Study Guideline for iOS Platform. In Distributed, Ambient and Pervasive Interactions, Norbert Streitz and Shiníchi Konomi (Eds.), Vol. 12203. Springer International Publishing, Cham, 223–243. https://doi.org/10.1007/978- 3- 030- 50344- 4_17 
[8] Yuuki Nishiyama and Kaoru Sezaki. 2021. Experience Sampling Tool for Repetitive Skills Training in Sports Using Voice User Interface. In Adjunct Proceedings of the 2021 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2021 ACM International Symposium on Wearable Computers (2021-09-21) (UbiComp ’21). Association for Computing Machinery, Virtual, USA, 54–55. https://doi.org/10.1145/3460418.3479283 
[9] Ivan Miguel Pires, Nuno M. Garcia, Nuno Pombo, Francisco Flórez-Revuelta, and Susanna Spinsante. 2018. Approach for the Development of a Framework for the Identification of Activities of Daily Living Using Sensors in Mobile Devices. Sensors 18, 2 (2018). https://doi.org/10.3390/s18020640 
[10] Tobii Pro AB. 2014. Tobii Pro Lab. http://www.tobiipro.com/ Computer software. 
[11] Kai Zhan, Steven Faux, and Fabio Ramos. 2015. Multi-scale Conditional Random Fields for first-person activity recognition on elders and disabled patients. Pervasive and Mobile Computing 16 (2015), 251–267. https://doi.org/10.1016/j. pmcj.2014.11.004 Selected Papers from the Twelfth Annual IEEE International Conference on Pervasive Computing and Communications (PerCom 2014).
